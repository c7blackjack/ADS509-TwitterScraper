{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/c7blackjack/ADS509-TwitterScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tweepy\r\n",
      "Version: 4.10.1\r\n",
      "Summary: Twitter library for Python\r\n",
      "Home-page: https://www.tweepy.org/\r\n",
      "Author: Joshua Roesslein\r\n",
      "Author-email: tweepy@googlegroups.com\r\n",
      "License: MIT\r\n",
      "Location: /Users/travis/opt/anaconda3/lib/python3.8/site-packages\r\n",
      "Requires: oauthlib, requests, requests-oauthlib\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports are for the Twitter section of the assignment\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# These imports are for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any packages used here\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import urllib\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need bring in our API keys. Since API keys should be kept secret, we'll keep them in a file called api_keys.py. This file should be stored in the directory where you store this notebook. The example file is provided for you on Blackboard. The example has API keys that are not functional, so you'll need to get Twitter credentials and replace the placeholder keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_keys import api_key, api_key_secret, bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter APIs are quite rich. Let's play around with some of the features before we dive into this section of the assignment. For our testing, it's convenient to have a small data set to play with. We will seed the code with the handle of John Chandler, one of the instructors in this course. His handle is @37chandler. Feel free to use a different handle if you would like to look at someone else's data.\n",
    "\n",
    "We will write code to explore a few aspects of the API:\n",
    "\n",
    "1. Pull some of the followers @37chandler.\n",
    "2. Explore response data, which gives us information about Twitter users.\n",
    "3. Pull the last few tweets by @37chandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"37chandler\"\n",
    "user_obj = client.get_user(username=handle)\n",
    "\n",
    "followers = client.get_users_followers(\n",
    "    # Learn about user fields here: \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user\n",
    "    user_obj.data.id, user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                   \"public_metrics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore these a bit. We'll start by printing out names, locations, following count, and followers count for these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave Renn lists 'None' as their location.\n",
      " Following: 42, Followers: 10.\n",
      "\n",
      "Lionel lists 'None' as their location.\n",
      " Following: 202, Followers: 204.\n",
      "\n",
      "Megan Randall lists 'None' as their location.\n",
      " Following: 141, Followers: 100.\n",
      "\n",
      "Jacob Salzman lists 'None' as their location.\n",
      " Following: 562, Followers: 134.\n",
      "\n",
      "twiter not fun lists 'None' as their location.\n",
      " Following: 221, Followers: 21.\n",
      "\n",
      "Hariettwilsonincarnate lists 'None' as their location.\n",
      " Following: 218, Followers: 60.\n",
      "\n",
      "Christian Tinsley lists 'None' as their location.\n",
      " Following: 2, Followers: 0.\n",
      "\n",
      "Steve lists 'I'm over here.' as their location.\n",
      " Following: 1592, Followers: 33.\n",
      "\n",
      "John O'Connor ðŸ‡ºðŸ‡¦ lists 'None' as their location.\n",
      " Following: 8, Followers: 1.\n",
      "\n",
      "CodeGrade lists 'Amsterdam' as their location.\n",
      " Following: 2820, Followers: 424.\n",
      "\n",
      "Cleverhood lists 'Providence, RI' as their location.\n",
      " Following: 2795, Followers: 3563.\n",
      "\n",
      "Regina ðŸš¶â€â™€ï¸ðŸš²ðŸŒ³ lists 'Minneapolis' as their location.\n",
      " Following: 2802, Followers: 3337.\n",
      "\n",
      "Eric Hallstrom lists 'Missoula, MT' as their location.\n",
      " Following: 464, Followers: 305.\n",
      "\n",
      "Tyler ðŸ“Š ðŸ• ðŸš² lists 'Minneapolis, MN' as their location.\n",
      " Following: 528, Followers: 83.\n",
      "\n",
      "The Center for Community Ownership (CCO) lists 'None' as their location.\n",
      " Following: 53, Followers: 41.\n",
      "\n",
      "Deepak Chauhan lists 'None' as their location.\n",
      " Following: 450, Followers: 25.\n",
      "\n",
      "Patsy lists 'Seattle, WA' as their location.\n",
      " Following: 156, Followers: 15.\n",
      "\n",
      "andrew lists 'St Paul, MN' as their location.\n",
      " Following: 1413, Followers: 461.\n",
      "\n",
      "Ada Smith lists 'None' as their location.\n",
      " Following: 274, Followers: 198.\n",
      "\n",
      "Stacey Burns lists 'Minneapolis Witch District' as their location.\n",
      " Following: 4585, Followers: 10880.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 20\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    following_count = user.public_metrics['following_count']\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    print(f\"{user.name} lists '{user.location}' as their location.\")\n",
    "    print(f\" Following: {following_count}, Followers: {followers_count}.\")\n",
    "    print()\n",
    "    \n",
    "    if idx >= (num_to_print - 1) :\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the person who follows this handle who has the most followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WedgeLIVE\n",
      "{'followers_count': 14179, 'following_count': 2222, 'tweet_count': 56103, 'listed_count': 218}\n"
     ]
    }
   ],
   "source": [
    "max_followers = 0\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    if followers_count > max_followers :\n",
    "        max_followers = followers_count\n",
    "        max_follower_user = user\n",
    "\n",
    "        \n",
    "print(max_follower_user)\n",
    "print(max_follower_user.public_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull some more user fields and take a look at them. The fields can be specified in the user_fields argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_user(id=user_obj.data.id,\n",
    "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                                       \"verified\",\"public_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for name we have John Chandler\n",
      "for verified we have False\n",
      "for public_metrics we have {'followers_count': 193, 'following_count': 589, 'tweet_count': 997, 'listed_count': 3}\n",
      "for description we have He/Him. Data scientist, urban cyclist, educator, erstwhile frisbee player. \n",
      "\n",
      "Â¯\\_(ãƒ„)_/Â¯\n",
      "for id we have 33029025\n",
      "for created_at we have 2009-04-18 22:08:22+00:00\n",
      "for username we have 37chandler\n",
      "for profile_image_url we have https://pbs.twimg.com/profile_images/2680483898/b30ae76f909352dbae5e371fb1c27454_normal.png\n",
      "for location we have MN\n"
     ]
    }
   ],
   "source": [
    "for field, value in response.data.items() :\n",
    "    print(f\"for {field} we have {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a few questions for you about the user object.\n",
    "\n",
    "Q: How many fields are being returned in the response object?\n",
    "\n",
    "A: There are 9 fields being returned\n",
    "    \n",
    "Q: Are any of the fields within the user object non-scalar? (I.e., more complicated than a simple data type like integer, float, string, boolean, etc.)\n",
    "\n",
    "A: Yes Public Metrics is a Dictionary\n",
    "    \n",
    "Q: How many friends, followers, and tweets does this user have?\n",
    "\n",
    "A: {Friends: 589, Followers: 193, Tweets: 99}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569760631548690437\n",
      "RT @dtmooreeditor: So there's a particular quirk of English grammar that I've always found quite endearing: the exocentric verb-noun compouâ€¦\n",
      "\n",
      "1569155273742327811\n",
      "As a Minneapolis person, I knew we had Toronto beat, but I didn't realize Portland had us beat: https://t.co/xrx5mOFcWK.\n",
      "\n",
      "But @nytimes, c'mon! https://t.co/M9mBWhdgsj\n",
      "\n",
      "1568982292923826176\n",
      "RT @wonderofscience: Amazing lenticular cloud over Mount Fuji\n",
      "\n",
      "Credit: Iurie Belegurschi\n",
      "https://t.co/0mUxl28H9U\n",
      "\n",
      "1568242374085869570\n",
      "RT @depthsofwiki: lots of memes about speedy wikipedia editors â€” quick thread about what went down on wikipedia in the minutes after her deâ€¦\n",
      "\n",
      "1568074978754703361\n",
      "@DrLaurenWilson @leighradwood @MaritsaGeorgiou @Walgreens I could not possibly agree more with this sentiment. Compared to almost any other primary care I've received, they are great.\n",
      "\n",
      "1567530169686196224\n",
      "@DrLaurenWilson @MaritsaGeorgiou @Walgreens For those who have access to Curry Health Center on campus, you can get a bivalent booster in 15 minutes from their delightful staff.\n",
      "\n",
      "1567511181526708224\n",
      "RT @shes_the_maNN1: I canâ€™t describe how ancient this makes me feel. https://t.co/a1IvELjOFY\n",
      "\n",
      "1567510612665864193\n",
      "RT @AngryBlackLady: this is hilarious\n",
      "\n",
      "1566031636457725953\n",
      "RT @MarkJacob16: With all the arguments over whether MAGA Republicans are fascists, I reread William Shirerâ€™s â€œThe Rise and Fall of the Thiâ€¦\n",
      "\n",
      "1563737816219000832\n",
      "RT @wonderofscience: The Milky Way galaxy and a phenomenon known as \"airglow\" seen from the International Space Station. https://t.co/bOLt8â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.get_users_tweets(user_obj.data.id)\n",
    "\n",
    "# By default, only the ID and text fields of each Tweet will be returned\n",
    "for idx, tweet in enumerate(response.data) :\n",
    "    print(tweet.id)\n",
    "    print(tweet.text)\n",
    "    print()\n",
    "    \n",
    "    if idx > 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Follower Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section of the assignment, we will pull information about the followers of your two artists. We've seen above how to pull a set of followers using client.get_users_followers. This function has a parameter, max_results, that we can use to change the number of followers that we pull. Unfortunately, we can only pull 1000 followers at a time, which means we will need to handle the pagination of our results.\n",
    "\n",
    "The return object has the .data field, where the results will be found. It also has .meta, which we use to select the next \"page\" in the results using the next_token result. I will illustrate the ideas using our user from above.\n",
    "\n",
    "## Rate Limiting\n",
    "\n",
    "Twitter limits the rates at which we can pull data, as detailed in this guide. We can make 15 user requests per 15 minutes, meaning that we can pull 4x15x100 = 60000 users per hour. I illustrate the handling of rate limiting below, though whether or not you hit that part of the code depends on your value of handle.\n",
    "\n",
    "In the below example, I'll pull all the followers, 25 at a time. (We're using 25 to illustrate the idea; when you do this set the value to 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_followers = []\n",
    "pulls = 0\n",
    "max_pulls = 100\n",
    "next_token = None\n",
    "\n",
    "while True :\n",
    "\n",
    "    followers = client.get_users_followers(\n",
    "        user_obj.data.id, \n",
    "        max_results=1000, # when you do this for real, set this to 1000!\n",
    "        pagination_token = next_token,\n",
    "        user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                     \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                     \"verified\",\"public_metrics\"]\n",
    "    )\n",
    "    pulls += 1\n",
    "    \n",
    "    for follower in followers.data : \n",
    "        follower_row = (follower.id,follower.name,follower.created_at,follower.description)\n",
    "        handle_followers.append(follower_row)\n",
    "    \n",
    "    if 'next_token' in followers.meta and pulls < max_pulls :\n",
    "        next_token = followers.meta['next_token']\n",
    "    else : \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Twitter Data for Your Artists\n",
    "\n",
    "Now let's take a look at your artists and see how long it is going to take to pull all their followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 11.43 hours to pull all 685515 followers for nfrealmusic. \n",
      "It would take 3.49 hours to pull all 209489 followers for atmosphere. \n"
     ]
    }
   ],
   "source": [
    "artists = dict()\n",
    "\n",
    "for handle in ['nfrealmusic','atmosphere'] : \n",
    "    user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"])\n",
    "    artists[handle] = (user_obj.data.id, \n",
    "                       handle,\n",
    "                       user_obj.data.public_metrics['followers_count'])\n",
    "    \n",
    "\n",
    "for artist, data in artists.items() : \n",
    "    print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {artist}. \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the \"twitter\" folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
    "\n",
    "if not os.path.isdir(\"twitter\") : \n",
    "    #shutil.rmtree(\"twitter/\")\n",
    "    os.mkdir(\"twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this following cells, build on the above code to pull some of the followers and their data for your two artists. As you pull the data, write the follower ids to a file called [artist name]_followers.txt in the \"twitter\" folder. For instance, for Cher I would create a file named cher_followers.txt. As you pull the data, also store it in an object like a list or a data frame.\n",
    "\n",
    "In addition to creating a file that only has follower IDs in it, you will create a file that includes user data. From the response object please extract and store the following fields:\n",
    "\n",
    "1. screen_name\n",
    "2. name\n",
    "3. id\n",
    "4. location\n",
    "5. followers_count\n",
    "6. friends_count\n",
    "7. description\n",
    "\n",
    "Store the fields with one user per row in a tab-delimited text file with the name [artist name]_follower_data.txt. For instance, for Cher I would create a file named cher_follower_data.txt.\n",
    "\n",
    "One note: the user's description can have tabs or returns in it, so make sure to clean those out of the description before writing them to the file. I've included some example code to do that below the stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_followers_to_pull = 100*1000 # feel free to use this to limit the number of followers you pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/travis\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd() + '/twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': '1U7T01MV6FPHEZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'HTJBT9J3FIQHEZZZ', 'previous_token': 'P4MQ92Q9QC6EGZZZ'}\n",
      "{'result_count': 1000, 'next_token': '19P3CBRFCI0HEZZZ', 'previous_token': 'IGPF8RAKJT5EGZZZ'}\n",
      "{'result_count': 1000, 'next_token': '5BJL367MLPLHEZZZ', 'previous_token': 'JNTTU6UNK5VEGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'V64C7DSLDDAHEZZZ', 'previous_token': 'UBJ2I3GGA6AEGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'I8F6FBCHL0V1EZZZ', 'previous_token': '2SR6JQ4CJ2LEGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'V5JBLKD944O1EZZZ', 'previous_token': '25TO94I9BF0UGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'PA4E71T5S0LHEZZZ', 'previous_token': 'QE8F7BDPRV7UGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'ST3LMPSCB4JHEZZZ', 'previous_token': 'LJO081Q847AEGZZZ'}\n",
      "{'result_count': 1000, 'next_token': '5SNHHFP44OI1EZZZ', 'previous_token': 'LCOQ88POLFCEGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'M5U6LU55E8BHEZZZ', 'previous_token': 'RHNJEJEPRJDUGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'LB39EO5OQ461EZZZ', 'previous_token': '7O1GJV8LHRKEGZZZ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 888 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': 'N2ARF2F1UNVHCZZZ', 'previous_token': 'HBUGJK3Q6NPUGZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'U465DTF46VOHCZZZ', 'previous_token': '7L1I900G1C0EIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'RR4HBTRQQBIHCZZZ', 'previous_token': 'NL6IA6GOQ47EIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'H7M9QR1897C1CZZZ', 'previous_token': 'IP0SJ85V5KDEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'MP2QHQ2UMJ51CZZZ', 'previous_token': 'ALRAKALGMSJUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'U58OG96P1UVHCZZZ', 'previous_token': 'CIDM5N769GQUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '8U40GRTFM6P1CZZZ', 'previous_token': '94CU2A0UVD0EIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'IQLL8D75BQJ1CZZZ', 'previous_token': '45GHEGUOA16UIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '6498D7655QDHCZZZ', 'previous_token': 'UJRKURGDKPCUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'DNM2H7FSEA81CZZZ', 'previous_token': '5MQL9FQ7Q9IEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '9L8STQ2K0I1HCZZZ', 'previous_token': 'B63E378MHTNUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'NL5I0S7VKDP1CZZZ', 'previous_token': '795RH48E05UUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'QKL0ACDIJLI1CZZZ', 'previous_token': '9GBQ9RUVCA6UIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'AM1VKJ9TI1BHCZZZ', 'previous_token': 'UCOGU4MCCUDUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'DKG7QC7TBL61CZZZ', 'previous_token': '48FMBFL3EUKEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'PB3R1QVF38VHCZZZ', 'previous_token': '9IQS1J7FLQPUIZZZ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 884 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': '1P90639T28QHCZZZ', 'previous_token': 'ACRNTC3VSR0EIZZZ'}\n",
      "{'result_count': 998, 'next_token': '845UD4VLK4N1CZZZ', 'previous_token': '81EQ3FO4UB5EIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '6EAQ5G95CGLHCZZZ', 'previous_token': '8B0I630NBV8UIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'K5NPQUUE4KK1CZZZ', 'previous_token': 'HAHO9R8UJJAEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '8ENBD9RI8CI1CZZZ', 'previous_token': 'NF97VV4HRBBUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'OB8MS8306GF1CZZZ', 'previous_token': 'KQPA95O3NNDUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'J503A0JARCD1CZZZ', 'previous_token': 'GTQV7QD0PFGUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'M387D1GBM4B1CZZZ', 'previous_token': 'GPEBDPRD4NIUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'L6EOGI0TCO91CZZZ', 'previous_token': 'GQACKMLB9VKUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'JUBVRPV9MK6HCZZZ', 'previous_token': '50K9LRFFJBMUIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '079BTGCFIC4HCZZZ', 'previous_token': 'EGHA41JR9BPEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '5MDB810AD82HCZZZ', 'previous_token': '37U2BSBPDRREIZZZ'}\n",
      "{'result_count': 1000, 'next_token': '97SICV2I7411CZZZ', 'previous_token': 'REL96BKLIRTEIZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'H1V2PG9H6NVHAZZZ', 'previous_token': '4A5GM8MUORUUIZZZ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 891 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': 'IQ21RAFH3FTHAZZZ', 'previous_token': '412M4GA9PC0EKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '60GG1CPS57R1AZZZ', 'previous_token': '09QB99U0SG2EKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'JCUFO0LFNVP1AZZZ', 'previous_token': '95E8O00RR04UKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '9PA45LOPB7NHAZZZ', 'previous_token': '2ECH7B61846UKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'NDVU44GVNVM1AZZZ', 'previous_token': '2UAOK383L88EKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '1RUIC9JJGBL1AZZZ', 'previous_token': 'TTKS6FAM849UKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'RFOADF72RVJHAZZZ', 'previous_token': 'E9DA2NLLFOAUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'SUSJR31KEFIHAZZZ', 'previous_token': 'U5IC7CS840CEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'O5A0IUIBN7H1AZZZ', 'previous_token': '0HTGCSDEHSDEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'UET5PKEGH3G1AZZZ', 'previous_token': 'QIFNT3U48OEUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'PCV242T4FNF1AZZZ', 'previous_token': '75DMRSL9ESFUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'C5VE7OU3HRE1AZZZ', 'previous_token': 'CBFJA845G8GUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '1RJNJIRAUJCHAZZZ', 'previous_token': '7DTPDQSNE4HUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '3MPDSOCHVVBHAZZZ', 'previous_token': 'BQKL1BOP1GJEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'O242UPN60VB1AZZZ', 'previous_token': '51T19HKK00KEKZZZ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 886 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': 'LB8UEHKH5VA1AZZZ', 'previous_token': 'GQ87RN7PV0KUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '30T1446KGJ91AZZZ', 'previous_token': 'D4UPK84MQ0LUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'GUJVF9E8EN81AZZZ', 'previous_token': '3CV9H7D7FCMUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '73FOUG5OQB6HAZZZ', 'previous_token': 'D9RH16JOH8NUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '1LERKRG2G75HAZZZ', 'previous_token': 'C9EQ3Q5A5KPEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'C12BS28TC301AZZZ', 'previous_token': '1L24PBOKFSQEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'E4RVG1N6FQOHAZZZ', 'previous_token': '9COBOHJDKGVUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'JSM483BASMJ1AZZZ', 'previous_token': 'B7O8OF5OG97EKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '5P7V6CIVMEHHAZZZ', 'previous_token': '7TKRP5VT3TCUKZZZ'}\n",
      "{'result_count': 999, 'next_token': 'FGN4022PMQGHAZZZ', 'previous_token': 'SJ10S7C59LEEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'K0M7S6BR8UG1AZZZ', 'previous_token': '93O7LEUK95FEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'OH9TUUQVP2F1AZZZ', 'previous_token': '5NQQIOKQN1FUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'TT6693LNEQEHAZZZ', 'previous_token': 'A6A8IORL71GUKZZZ'}\n",
      "{'result_count': 1000, 'next_token': 'VBD7V1ONLAC1AZZZ', 'previous_token': 'NUP1TFFEH5HEKZZZ'}\n",
      "{'result_count': 1000, 'next_token': '49CANIGHP261AZZZ', 'previous_token': 'G2TK522HATJUKZZZ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 890 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result_count': 1000, 'next_token': '2SVDM8HG4A11AZZZ', 'previous_token': 'OE3LGKNN6TPUKZZZ'}\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e5c788ef337f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     for response in tweepy.Paginator(client.get_users_followers,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                      \u001b[0muser_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                      \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/pagination.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pagination_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagination_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mget_users_followers\u001b[0;34m(self, id, user_auth, **params)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfollows\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfollowers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \"\"\"\n\u001b[0;32m-> 2081\u001b[0;31m         return self._make_request(\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/2/users/{id}/followers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             endpoint_parameters=(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrequest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         response = self.request(method, route, params=request_params,\n\u001b[0m\u001b[1;32m    127\u001b[0m                                 json=json, user_auth=user_auth)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m    110\u001b[0m                         )\n\u001b[1;32m    111\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_auth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         with self.session.request(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
    "\n",
    "handles = ['atmosphere']\n",
    "\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "user_data = dict() \n",
    "followers_data = dict()\n",
    "\n",
    "def convert_row(user):\n",
    "    user_id = str(user.id)\n",
    "    username = str(user.username)\n",
    "    name = str(user.name)\n",
    "    location = str(user.location)\n",
    "    followers_count = str(user.public_metrics['followers_count'])\n",
    "    following_count = str(user.public_metrics['following_count'])\n",
    "    description = user.description\n",
    "    description = description.replace(\"\\n\",\"|\")\n",
    "    description = description.replace(\"\\t\",\"\t\")\n",
    "    description = description.replace('\"','``')\n",
    "    \n",
    "    return '\\t'.join([user_id,username,name,location,followers_count,following_count,description]) + \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "for handle in handles :\n",
    "    user_data[handle] = [] # will be a list of lists\n",
    "    followers_data[handle] = [] # will be a simple list of IDs\n",
    "\n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for handle in handles :\n",
    "    user_obj = client.get_user(username=handle)\n",
    "\n",
    "    # Create the output file names \n",
    "    \n",
    "    followers_output_file = handle + \"_followers.txt\"\n",
    "    user_data_output_file = handle + \"_follower_data.txt\"\n",
    "    \n",
    "\n",
    "    for response in tweepy.Paginator(client.get_users_followers,\n",
    "                                     user_obj.data.id,\n",
    "                                     max_results=1000,\n",
    "                                     limit=100,\n",
    "                                     user_fields=['location','public_metrics','description']):\n",
    "        print(response.meta)            \n",
    "        for user in response.data : \n",
    "            follower_row = convert_row(user)\n",
    "\n",
    "            user_data[handle].append(follower_row)\n",
    "            followers_data[handle].append(user.id)\n",
    "            user_id = str(user.id)\n",
    "            f = open(followers_output_file,'a')\n",
    "            f.write(follower_row)\n",
    "            f.close()\n",
    "            f = open(user_data_output_file,'a')\n",
    "            f.write(user_id + '\\t')\n",
    "            f.close()\n",
    "\n",
    "    \n",
    "\n",
    "    # For each response object, extract the needed fields and store them in a dictionary or\n",
    "    # data frame. \n",
    "\n",
    "    # I recommend writing your results for every response. This isn't the most efficient option\n",
    "    # (since you're opening and closing the file regularly), but it ensures that your \n",
    "    # work is saved in case there is an issue with the API connection. \n",
    "    \n",
    "    # If you've pulled num_followers_to_pull, feel free to break out paged twitter API response\n",
    "\n",
    "\n",
    "        \n",
    "# Let's see how long it took to grab all follower IDs\n",
    "end_time = datetime.datetime.now()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = {'lukeccombs':\"https://www.azlyrics.com/l/lukecombs.html\",\n",
    "           'atmosphere':\"https://www.azlyrics.com/a/atmosphere.html\"} \n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Take a look at the robots.txt page on www.azlyrics.com. (You can read more about these pages here.) Is the scraping we are about to do allowed or disallowed by this page? How do you know?\n",
    "\n",
    "A: In reading the disallow and allow parts, It would seem that it is allowed to scrape pages but not the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items() :\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    thepage = urllib.request.urlopen(artist_page)\n",
    "    soup = BeautifulSoup(thepage,\"html.parser\")\n",
    "\n",
    "    for i in soup.find_all('div', attrs={'class' : 'listalbum-item'}):\n",
    "        lyrics_pages[artist].append(i.a['href'])\n",
    "\n",
    "    # now extract the links to lyrics pages from this page\n",
    "    # store the links `lyrics_pages` where the key is the artist and the\n",
    "    # value is a list of links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'lukeccombs': ['/lyrics/lukecombs/thewaysherides.html', '/lyrics/lukecombs/iknowsheaintready.html', '/lyrics/lukecombs/letthemoonshine.html', '/lyrics/lukecombs/shegotthebestofme.html', '/lyrics/lukecombs/sheriffyouwantto.html', '/lyrics/lukecombs/canigetanoutlaw.html', '/lyrics/lukecombs/outthere.html', '/lyrics/lukecombs/memoriesaremadeof.html', '/lyrics/lukecombs/lonelyone.html', '/lyrics/lukecombs/beercan.html', '/lyrics/lukecombs/hurricane.html', '/lyrics/lukecombs/onenumberaway.html', '/lyrics/lukecombs/donttemptme.html', '/lyrics/lukecombs/whenitrainsitpours.html', '/lyrics/lukecombs/thisonesforyou.html', '/lyrics/lukecombs/becarefulwhatyouwishfor.html', '/lyrics/lukecombs/igotawaywithyou.html', '/lyrics/lukecombs/honkytonkhighway.html', '/lyrics/lukecombs/houstonwegotaproblem.html', '/lyrics/lukecombs/mustvenevermetyou.html', '/lyrics/lukecombs/beautifulcrazy.html', '/lyrics/lukecombs/alongway.html', '/lyrics/lukecombs/shegotthebestofme.html', '/lyrics/lukecombs/beerneverbrokemyheart.html', '/lyrics/lukecombs/refrigeratordoor.html', '/lyrics/lukecombs/eventhoughimleaving.html', '/lyrics/lukecombs/lovinonyou.html', '/lyrics/lukecombs/moonovermexico.html', '/lyrics/lukecombs/12many.html', '/lyrics/lukecombs/bluecollarboys.html', '/lyrics/lukecombs/neweveryday.html', '/lyrics/lukecombs/reasons.html', '/lyrics/lukecombs/everylittlebithelps.html', '/lyrics/lukecombs/deartoday.html', '/lyrics/lukecombs/whatyouseeiswhatyouget.html', '/lyrics/lukecombs/doestome.html', '/lyrics/lukecombs/angelsworkinovertime.html', '/lyrics/lukecombs/alloveragain.html', '/lyrics/lukecombs/nothinglikeyou.html', '/lyrics/lukecombs/bettertogether.html', '/lyrics/lukecombs/sixfeetapart.html', '/lyrics/lukecombs/coldasyou.html', '/lyrics/lukecombs/theotherguy.html', '/lyrics/lukecombs/mykindafolk.html', '/lyrics/lukecombs/withoutyou.html', '/lyrics/lukecombs/foreverafterall.html', '/lyrics/lukecombs/dointhis.html', '/lyrics/lukecombs/anygivenfridaynight.html', '/lyrics/lukecombs/thekindoflovewemake.html', '/lyrics/lukecombs/ontheotherline.html', '/lyrics/lukecombs/outrunninyourmemory.html', '/lyrics/lukecombs/usedtowishiwas.html', '/lyrics/lukecombs/betterbackwhen.html', '/lyrics/lukecombs/tomorrowme.html', '/lyrics/lukecombs/aintfarfromit.html', '/lyrics/lukecombs/callme.html', '/lyrics/lukecombs/middleofsomewhere.html', '/lyrics/lukecombs/goinggoinggone.html', '/lyrics/lukecombs/dive.html', '/lyrics/lukecombs/evermine.html', '/lyrics/lukecombs/greystonechapel.html', '/lyrics/lukecombs/joe.html', '/lyrics/lukecombs/letsjustbefriends.html', '/lyrics/lukecombs/loveyouanyway.html', '/lyrics/lukecombs/onlylonelyone.html', '/lyrics/lukecombs/seemenow.html', '/lyrics/lukecombs/seminolewind.html', '/lyrics/lukecombs/southonya.html', '/lyrics/lukecombs/thegreatdivide.html', '/lyrics/lukecombs/thesefourwalls.html', '/lyrics/lukecombs/usedtoyou.html', '/lyrics/lukecombs/westilldrinkbeer.html'], 'atmosphere': ['/lyrics/atmosphere/1597.html', '/lyrics/atmosphere/briefdescription.html', '/lyrics/atmosphere/currentstatus.html', '/lyrics/atmosphere/complications.html', '/lyrics/atmosphere/430am.html', '/lyrics/atmosphere/adjust.html', '/lyrics/atmosphere/clay.html', '/lyrics/atmosphere/987291.html', '/lyrics/atmosphere/soundisvibration.html', '/lyrics/atmosphere/multiples.html', '/lyrics/atmosphere/scapegoat.html', '/lyrics/atmosphere/odetothemodernmanlightningblend.html', '/lyrics/atmosphere/wnd.html', '/lyrics/atmosphere/multiplesreprise.html', '/lyrics/atmosphere/cavedin.html', '/lyrics/atmosphere/cuandolimpiaelhumo.html', '/lyrics/atmosphere/theouternet.html', '/lyrics/atmosphere/overcast.html', '/lyrics/atmosphere/primer.html', '/lyrics/atmosphere/godsbathroomfloor.html', '/lyrics/atmosphere/busbenches.html', '/lyrics/atmosphere/peaking.html', '/lyrics/atmosphere/betweenthelines.html', '/lyrics/atmosphere/liketoday.html', '/lyrics/atmosphere/tearsforthesheep.html', '/lyrics/atmosphere/gunsandcigarettes.html', '/lyrics/atmosphere/donteverfuckingquestionthat.html', '/lyrics/atmosphere/itgoes.html', '/lyrics/atmosphere/ifiwassantaclaus.html', '/lyrics/atmosphere/aspiringsociopath.html', '/lyrics/atmosphere/freeordead.html', '/lyrics/atmosphere/partyforthefighttowrite.html', '/lyrics/atmosphere/mamahadababyandhisheadpoppedoff.html', '/lyrics/atmosphere/theyreallgonnalaughyou.html', '/lyrics/atmosphere/lostandfound.html', '/lyrics/atmosphere/thewomanwiththetattooedhands.html', '/lyrics/atmosphere/nothingbutsunshine.html', '/lyrics/atmosphere/homecoming.html', '/lyrics/atmosphere/onemosphere.html', '/lyrics/atmosphere/thebassandthemovement.html', '/lyrics/atmosphere/giveme.html', '/lyrics/atmosphere/fuckyoulucy.html', '/lyrics/atmosphere/hair.html', '/lyrics/atmosphere/godlovesugly.html', '/lyrics/atmosphere/asongaboutafriend.html', '/lyrics/atmosphere/flesh.html', '/lyrics/atmosphere/savestheday.html', '/lyrics/atmosphere/lovelife.html', '/lyrics/atmosphere/breathing.html', '/lyrics/atmosphere/vampires.html', '/lyrics/atmosphere/agirlnamedhope.html', '/lyrics/atmosphere/godlovesuglyreprise.html', '/lyrics/atmosphere/modernmanshustle.html', '/lyrics/atmosphere/oneofakind.html', '/lyrics/atmosphere/blamegame.html', '/lyrics/atmosphere/shrapnel.html', '/lyrics/atmosphere/history.html', '/lyrics/atmosphere/tryingtofindabalance.html', '/lyrics/atmosphere/birdsingswhythecagediknow.html', '/lyrics/atmosphere/reflections.html', '/lyrics/atmosphere/gottalottawalls.html', '/lyrics/atmosphere/thekeystolifevs15minutesoffame.html', '/lyrics/atmosphere/apple.html', '/lyrics/atmosphere/suicidegirls.html', '/lyrics/atmosphere/jason.html', '/lyrics/atmosphere/catsvanbags.html', '/lyrics/atmosphere/losangeles.html', '/lyrics/atmosphere/liftherpullher.html', '/lyrics/atmosphere/shoes.html', '/lyrics/atmosphere/nationaldisgrace.html', '/lyrics/atmosphere/denvemolorado.html', '/lyrics/atmosphere/liquorlylescooljuly.html', '/lyrics/atmosphere/goodtimessickpimpin.html', '/lyrics/atmosphere/inmycontinental.html', '/lyrics/atmosphere/alwayscomingbackhometoyou.html', '/lyrics/atmosphere/shhh.html', '/lyrics/atmosphere/dmfd.html', '/lyrics/atmosphere/mysongs.html', '/lyrics/atmosphere/sepsevengameshowtheme.html', '/lyrics/atmosphere/roundandround.html', '/lyrics/atmosphere/tracksmart.html', '/lyrics/atmosphere/chokingonawishbone.html', '/lyrics/atmosphere/thejackpotsweptaway.html', '/lyrics/atmosphere/thestickup.html', '/lyrics/atmosphere/lylelovette.html', '/lyrics/atmosphere/higherliving.html', '/lyrics/atmosphere/tothebreakofsean.html', '/lyrics/atmosphere/deerwolf.html', '/lyrics/atmosphere/mollycool.html', '/lyrics/atmosphere/dungeonsanddragons.html', '/lyrics/atmosphere/anterlude.html', '/lyrics/atmosphere/advancedcommunications.html', '/lyrics/atmosphere/atallsevenandseven.html', '/lyrics/atmosphere/32reddog.html', '/lyrics/atmosphere/theabusingoftherib.html', '/lyrics/atmosphere/writenowmultiplesno4.html', '/lyrics/atmosphere/iwishthosecatsfobiawouldgivemesomefreeshoes.html', '/lyrics/atmosphere/heart.html', '/lyrics/atmosphere/industrialwarfare.html', '/lyrics/atmosphere/travelremix.html', '/lyrics/atmosphere/7thstentry.html', '/lyrics/atmosphere/multiplesrepriseremix.html', '/lyrics/atmosphere/funnycolorsinmymushroomtrails.html', '/lyrics/atmosphere/dubs.html', '/lyrics/atmosphere/substanceabuse.html', '/lyrics/atmosphere/thearrival.html', '/lyrics/atmosphere/panicattackthepa.html', '/lyrics/atmosphere/watchout.html', '/lyrics/atmosphere/musicalchairs.html', '/lyrics/atmosphere/sayheythere53168.html', '/lyrics/atmosphere/hockeyhair.html', '/lyrics/atmosphere/bam.html', '/lyrics/atmosphere/pourmeanother.html', '/lyrics/atmosphere/smartwentcrazy.html', '/lyrics/atmosphere/angelfacemultiples5vstravel4.html', '/lyrics/atmosphere/thatnight.html', '/lyrics/atmosphere/getflywhatifjesusforgottoputyouontheguestlist.html', '/lyrics/atmosphere/littleman.html', '/lyrics/atmosphere/sayheythere74944.html', '/lyrics/atmosphere/panicattack.html', '/lyrics/atmosphere/theycallit.html', '/lyrics/atmosphere/youmakemewanna.html', '/lyrics/atmosphere/secret.html', '/lyrics/atmosphere/spaghettistrapped.html', '/lyrics/atmosphere/hornyponycornhorns.html', '/lyrics/atmosphere/sunshine.html', '/lyrics/atmosphere/thenumberone.html', '/lyrics/atmosphere/rftc.html', '/lyrics/atmosphere/mattress.html', '/lyrics/atmosphere/dontforget.html', '/lyrics/atmosphere/peyote.html', '/lyrics/atmosphere/partyoverhere.html', '/lyrics/atmosphere/makethesuncomeout.html', '/lyrics/atmosphere/therooster.html', '/lyrics/atmosphere/lyndaleavenueusersmanual.html', '/lyrics/atmosphere/dontstop.html', '/lyrics/atmosphere/66thstreet.html', '/lyrics/atmosphere/theyallgetmadatyou.html', '/lyrics/atmosphere/beautiful.html', '/lyrics/atmosphere/hathisoneisaboutalcoholtoo.html', '/lyrics/atmosphere/ygm.html', '/lyrics/atmosphere/littlemathyou.html', '/lyrics/atmosphere/fullmoon.html', '/lyrics/atmosphere/thethingsthathateus.html', '/lyrics/atmosphere/jewelry.html', '/lyrics/atmosphere/getittogether.html', '/lyrics/atmosphere/domesticdog.html', '/lyrics/atmosphere/crewedup.html', '/lyrics/atmosphere/whattheysittingfor.html', '/lyrics/atmosphere/thatsnotbeefthatspork.html', '/lyrics/atmosphere/theoldstyle.html', '/lyrics/atmosphere/youplayedyourself.html', '/lyrics/atmosphere/roadtoriches.html', '/lyrics/atmosphere/liketherestofus.html', '/lyrics/atmosphere/puppets.html', '/lyrics/atmosphere/theskinny.html', '/lyrics/atmosphere/dreamer.html', '/lyrics/atmosphere/shouldaknown.html', '/lyrics/atmosphere/you.html', '/lyrics/atmosphere/painting.html', '/lyrics/atmosphere/yourglasshouse.html', '/lyrics/atmosphere/yesterday.html', '/lyrics/atmosphere/guarantees.html', '/lyrics/atmosphere/me.html', '/lyrics/atmosphere/wildwildhorses.html', '/lyrics/atmosphere/cantbreak.html', '/lyrics/atmosphere/thewaitress.html', '/lyrics/atmosphere/inhermusicbox.html', '/lyrics/atmosphere/vanitysick.html', '/lyrics/atmosphere/keyboard.html', '/lyrics/atmosphere/lessone.html', '/lyrics/atmosphere/gooddaddy.html', '/lyrics/atmosphere/carrymehome.html', '/lyrics/atmosphere/happymess.html', '/lyrics/atmosphere/notanotherday.html', '/lyrics/atmosphere/cmon.html', '/lyrics/atmosphere/theyalwaysknow.html', '/lyrics/atmosphere/theropes.html', '/lyrics/atmosphere/whitenoise.html', '/lyrics/atmosphere/feelgoodhitofthesummerpart2.html', '/lyrics/atmosphere/mothersday.html', '/lyrics/atmosphere/milliefelloffthefireescape.html', '/lyrics/atmosphere/untilthenipplesgone.html', '/lyrics/atmosphere/themajorleagues.html', '/lyrics/atmosphere/scalp.html', '/lyrics/atmosphere/thebestday.html', '/lyrics/atmosphere/americareful.html', '/lyrics/atmosphere/hope.html', '/lyrics/atmosphere/theloserwins.html', '/lyrics/atmosphere/shotgun.html', '/lyrics/atmosphere/commodities.html', '/lyrics/atmosphere/thenumbernone.html', '/lyrics/atmosphere/freefallin.html', '/lyrics/atmosphere/toallmyfriends.html', '/lyrics/atmosphere/mykey.html', '/lyrics/atmosphere/thelasttosay.html', '/lyrics/atmosphere/became.html', '/lyrics/atmosphere/justforshow.html', '/lyrics/atmosphere/shesenough.html', '/lyrics/atmosphere/badbaddaddy.html', '/lyrics/atmosphere/milleniumdodo.html', '/lyrics/atmosphere/whoillneverbe.html', '/lyrics/atmosphere/idontneedbrighterdays.html', '/lyrics/atmosphere/aintnobody.html', '/lyrics/atmosphere/yournamehere.html', '/lyrics/atmosphere/ifyoucansavemenow.html', '/lyrics/atmosphere/somethingso.html', '/lyrics/atmosphere/mynotes.html', '/lyrics/atmosphere/millenniumdodo2.html', '/lyrics/atmosphere/cutyoudown.html', '/lyrics/atmosphere/camerathief.html', '/lyrics/atmosphere/arthurssong.html', '/lyrics/atmosphere/theworldmightnotlivethroughthenight.html', '/lyrics/atmosphere/starshapedheart.html', '/lyrics/atmosphere/iloveyoulikeabrother.html', '/lyrics/atmosphere/southsiders.html', '/lyrics/atmosphere/bitter.html', '/lyrics/atmosphere/mrsinterpret.html', '/lyrics/atmosphere/fortunate.html', '/lyrics/atmosphere/kanyewest.html', '/lyrics/atmosphere/weaintgonnadietoday.html', '/lyrics/atmosphere/myladygottwomen.html', '/lyrics/atmosphere/flicker.html', '/lyrics/atmosphere/januaryonlakestreet.html', '/lyrics/atmosphere/letmeknowthatyouknowwhatyouwantnow.html', '/lyrics/atmosphere/shedontknowwhysheloveit.html', '/lyrics/atmosphere/hell.html', '/lyrics/atmosphere/idontneednofancyshit.html', '/lyrics/atmosphere/idiot.html', '/lyrics/atmosphere/preludetohell.html', '/lyrics/atmosphere/likeafire.html', '/lyrics/atmosphere/ringo.html', '/lyrics/atmosphere/besos.html', '/lyrics/atmosphere/pureevil.html', '/lyrics/atmosphere/perfect.html', '/lyrics/atmosphere/seismicwaves.html', '/lyrics/atmosphere/nexttoyou.html', '/lyrics/atmosphere/theshitthatwevebeenthrough.html', '/lyrics/atmosphere/whenthelightsgoout.html', '/lyrics/atmosphere/nobiggie.html', '/lyrics/atmosphere/everything.html', '/lyrics/atmosphere/chasingnewyork.html', '/lyrics/atmosphere/sugar.html', '/lyrics/atmosphere/fishingblues.html', '/lyrics/atmosphere/wontlookback.html', '/lyrics/atmosphere/anybodythativeknown.html', '/lyrics/atmosphere/stillbehere.html', '/lyrics/atmosphere/alonghello.html', '/lyrics/atmosphere/jerome.html', '/lyrics/atmosphere/stopwatch.html', '/lyrics/atmosphere/virgo.html', '/lyrics/atmosphere/delicate.html', '/lyrics/atmosphere/drown.html', '/lyrics/atmosphere/anymore.html', '/lyrics/atmosphere/earring.html', '/lyrics/atmosphere/trim.html', '/lyrics/atmosphere/specificity.html', '/lyrics/atmosphere/mijo.html', '/lyrics/atmosphere/randymosh.html', '/lyrics/atmosphere/graffiti.html', '/lyrics/atmosphere/makeitallbetteragain.html', '/lyrics/atmosphere/sleepingonthebrightside.html', '/lyrics/atmosphere/bdemakaska.html', '/lyrics/atmosphere/pushplay.html', '/lyrics/atmosphere/postallady.html', '/lyrics/atmosphere/loveeachother.html', '/lyrics/atmosphere/romance.html', '/lyrics/atmosphere/dearlybeloved.html', '/lyrics/atmosphere/thehandsoftime.html', '/lyrics/atmosphere/whenever.html', '/lyrics/atmosphere/lovely.html', '/lyrics/atmosphere/sonofabyss.html', '/lyrics/atmosphere/youregonnago.html', '/lyrics/atmosphere/theceiling.html', '/lyrics/atmosphere/wheretheroadforks.html', '/lyrics/atmosphere/spaceissafe.html', '/lyrics/atmosphere/shelovesmynot.html', '/lyrics/atmosphere/thenewpeople.html', '/lyrics/atmosphere/thefutureisdisgusting.html', '/lyrics/atmosphere/doubletown.html', '/lyrics/atmosphere/stardust.html', '/lyrics/atmosphere/blotteracidrefluxsyndrome.html', '/lyrics/atmosphere/partycrashers.html', '/lyrics/atmosphere/sleepapnea.html', '/lyrics/atmosphere/fleetwood.html', '/lyrics/atmosphere/something.html', '/lyrics/atmosphere/crumbs.html', '/lyrics/atmosphere/woes.html', '/lyrics/atmosphere/strung.html', '/lyrics/atmosphere/clocked.html', '/lyrics/atmosphere/sleepless.html', '/lyrics/atmosphere/distances.html', '/lyrics/atmosphere/carousel.html', '/lyrics/atmosphere/vanish.html', '/lyrics/atmosphere/pressed.html', '/lyrics/atmosphere/skull.html', '/lyrics/atmosphere/nekst.html', '/lyrics/atmosphere/barcade.html', '/lyrics/atmosphere/barfood.html', '/lyrics/atmosphere/bobseger.html', '/lyrics/atmosphere/byyourside.html', '/lyrics/atmosphere/earblister.html', '/lyrics/atmosphere/finerthings.html', '/lyrics/atmosphere/fireflies.html', '/lyrics/atmosphere/minnesotanice.html', '/lyrics/atmosphere/mybesthalf.html', '/lyrics/atmosphere/themuahonyourcheek.html', '/lyrics/atmosphere/thislonelyrose.html', '/lyrics/atmosphere/tryingtofly.html', '/lyrics/atmosphere/windows.html']})\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lukeccombs we have 72.\n",
      "The full pull will take for this artist will take 0.2 hours.\n",
      "For atmosphere we have 310.\n",
      "The full pull will take for this artist will take 0.86 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/travis/twitter\n",
      "/Users/travis/twitter/lyrics\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd() + \"/lyrics\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>\"The Way She Rides\"</b>\n",
      "<div>\n",
      "<!-- Usage of azlyrics.com content by any third-party lyrics provider is prohibited by our licensing agreement. Sorry about that. -->\r\n",
      "Screamed into the driveway, with my foot stuck on the gas<br/>\n",
      "Had the Crown mixed up in a Dixie cup, when she hopped into the cab<br/>\n",
      "She said \"Boy what you got in store, what we getting into tonight?\"<br/>\n",
      "Hell, I don't care, I'll go anywhere, just as long as you're by my side<br/>\n",
      "<br/>\n",
      "<i>[Chorus:]</i><br/>\n",
      "Whether she's sitting shotgun with her hair done<br/>\n",
      "Or with the seat leaned back with the radio on<br/>\n",
      "Or the backstreets, in the backseat<br/>\n",
      "Those baby blue eyes staring back at me<br/>\n",
      "Whether it's in park or it's in drive<br/>\n",
      "I love the way she rides<br/>\n",
      "<br/>\n",
      "So we hit that ground a running throwing caution into the wind<br/>\n",
      "Well it wasn't long before that drink was gone, and she was shaking that ice again<br/>\n",
      "She said find somewhere dark where we can park, and put a hurting on this bottle<br/>\n",
      "So we said goodnight to the city lights, and kicked in full throttle<br/>\n",
      "<br/>\n",
      "<i>[Chorus]</i><br/>\n",
      "<br/>\n",
      "<i>[Chorus]</i>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "## Code to test finding first instance of a div with no class and no id\n",
    "\n",
    "thepage = requests.get('https://www.azlyrics.com/lyrics/lukecombs/thewaysherides.html')\n",
    "soup = BeautifulSoup(thepage.content,\"html.parser\")\n",
    "\n",
    "tag1 = soup.findAll('b')\n",
    "tag2 = soup.findAll('div', attrs={'class': None, 'id': None})\n",
    "\n",
    "print(tag1[1].extract())\n",
    "print(tag2[0].extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/travis/twitter/lyrics\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "for artist in lyrics_pages :\n",
    "\n",
    "    # Use this space to carry out the following steps: \n",
    "    \n",
    "    # 1. Build a subfolder for the artist\n",
    "    #if os.path.isdir(artist) : \n",
    "        #shutil.rmtree(artist + \"/\")\n",
    "    #os.mkdir(artist)\n",
    "    # 2. Iterate over the lyrics pages\n",
    "    for artist, lp in lyrics_pages.items():\n",
    "        for i in lp:\n",
    "            #print(os.getcwd() + \"/\" + artist + \"/\" + )\n",
    "            path = url_stub + i\n",
    "            r = requests.get(path)\n",
    "            time.sleep(5 + 10*random.random())\n",
    "        # 3. Request the lyrics page. \n",
    "            # Don't forget to add a line like `time.sleep(5 + 10*random.random())`\n",
    "            # to sleep after making the request\n",
    "        # 4. Extract the title and lyrics from the page.\n",
    "            soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "\n",
    "            title = soup.findAll('b')\n",
    "            title = str(title[1])\n",
    "            lyrics = soup.findAll('div', attrs={'class': None, 'id': None})\n",
    "            lyrics = str(lyrics[0])\n",
    "            \n",
    "            path = os.getcwd()\n",
    "            filename = generate_filename_from_link(i)\n",
    "\n",
    "            f = open(os.getcwd() + \"/\" + artist + \"/\" + filename,'w')\n",
    "            f.write(title)\n",
    "            f.write('\\n')\n",
    "            f.write('\\n')\n",
    "            f.write(lyrics)\n",
    "            f.close()\n",
    "        \n",
    "\n",
    "        \n",
    "    # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url`\n",
    "    #    to generate the filename. \n",
    "    \n",
    "    # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 2.74 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/travis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see two artist handles: lukecombs and atmosphere.\n"
     ]
    }
   ],
   "source": [
    "twitter_files = os.listdir(\"twitter\")\n",
    "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
    "artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
    "\n",
    "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lukecombs', 'atmosphere', '.ipynb']\n",
      "['lukecombs', 'atmosphere']\n"
     ]
    }
   ],
   "source": [
    "print(artist_handles)\n",
    "artist_handles.pop()\n",
    "print(artist_handles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see 108009 in your follower file for lukecombs, assuming a header row.\n",
      "We have 10 data rows for lukecombs in the follower data file.\n",
      "For lukecombs we have 0 unique locations.\n",
      "For lukecombs we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "We see 85097 in your follower file for atmosphere, assuming a header row.\n",
      "We have 10 data rows for atmosphere in the follower data file.\n",
      "For atmosphere we have 0 unique locations.\n",
      "For atmosphere we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_handles :\n",
    "    follower_file = artist + \"_followers.txt\"\n",
    "    follower_data_file = artist + \"_followers_data.txt\"\n",
    "    \n",
    "    ids = open(\"twitter/\" + follower_file,'r').readlines()\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
    "    \n",
    "    with open(\"twitter/\" + follower_data_file,'r') as infile :\n",
    "        \n",
    "        # check the headers\n",
    "        headers = infile.readline().split(\"\\t\")\n",
    "        \n",
    "#         print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
    "#         print(\" : \".join(headers))\n",
    "        \n",
    "        description_words = []\n",
    "        locations = set()\n",
    "        \n",
    "        \n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            try : \n",
    "                locations.add(line[3])            \n",
    "                description_words.extend(words(line[6]))\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        \n",
    "\n",
    "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
    "        print(\"Here are the five most common words:\")\n",
    "        print(Counter(description_words).most_common(5))\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For atmosphere we have 310 files.\n",
      "For atmosphere we have roughly 193092 words, 10846 are unique.\n",
      "For lukeccombs we have 71 files.\n",
      "For lukeccombs we have roughly 26369 words, 2052 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
